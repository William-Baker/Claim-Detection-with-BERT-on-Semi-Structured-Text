{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","# Import all the dependencies\n","import pandas as pd\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer\n","from pytorch_pretrained_bert import BertAdam\n","from sklearn.preprocessing import OrdinalEncoder\n","from statistics import mean\n","from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import numpy as np\n","from statistics import mean\n","from sklearn.metrics import f1_score\n","from common import BERTClassification\n","\n","\n","\n","\n","# Select the MODE to run in, these correspond to the 4 claim detection tasks: UT, TT, TU and UU\n","# Simply paste the string in from the options bellow to change the task\n","# TT : 'Twitter Pretrained'\n","# UT : 'Twitter Default'\n","# TU : 'UKP Pretrained'\n","# UU : 'UKP Default'\n","MODE = 'Twitter Pretrained'\n","\n","# Here the Hyper parameters for the model are chosen\n","MAX_SEQ_LENGTH = 64\n","BATCH_SIZE = 16\n","LEARNING_RATE = 2e-5\n","EPOCHS = 4\n","BINARIZE_LABELS = True # Whethere to binarise the UKP dataset from (For,Against,None) to (Claim/No Claim)\n","\n","# The bert model loaded in the Default case - for a fair comparision the BERT MODEL used in each task should be the same\n","BERT_MODEL = \"bert-base-uncased\" # bert-base-uncased bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese\n","\n","\n","# Configure which pre-trained model to load, if any\n","if MODE=='UKP Default': \n","    pre_trained_model_path = None\n","elif MODE=='UKP Pretrained':\n","    pre_trained_model_path = 'Trained Models/UKP/NSP/2022-03-12 16:38:17.119496 step: 20850'\n","elif MODE=='Twitter Default':\n","    pre_trained_model_path = None\n","elif MODE=='Twitter Pretrained':\n","    pre_trained_model_path = 'Trained Models/Twitter/NSP/2022-04-01 18:58:41.246851 step: 46515'\n","\n","\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["/home/w/Documents/KCL_Project/Clean/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1658: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n  warnings.warn(\n"]}],"source":["\n","if MODE=='UKP Default': # Write the logs and models to the appropriate folder\n","    log_dir = 'Trained Models/UKP/Claim Detection Default/'\n","elif MODE=='UKP Pretrained':\n","    log_dir='Trained Models/UKP/Claim Detection Pretrained/'\n","elif MODE=='Twitter Default':\n","    log_dir='Trained Models/Twitter/Claim Detection Default/'\n","elif MODE=='Twitter Pretrained':\n","    log_dir='Trained Models/Twitter/Claim Detection Pretrained/'\n","\n","\n","summary_writer = SummaryWriter(log_dir + 'runs') # Create the tensorboard logger\n","\n","# This is where we check if the system has a GPU, if so use it\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set the seed for all random number generators, this makes the results more reproducible\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","\n","\n","\n","# Load the appropriate dataset depending on the mode\n","if 'Twitter' in MODE:\n","    df = pd.read_pickle('../datasets/labelled tweets.pkl')\n","else:\n","    df = pd.read_excel('../datasets/UKP Claim Detection.xlsx')\n","    if BINARIZE_LABELS:\n","        df['y'] = df['y'].apply(lambda x: 'NoArgument' if x=='NoArgument' else 'Argument' )\n","\n","\n","if MODE == 'Twitter Pretrained': # Only use the Twitter specific vocabulary if we've pre-trained on the Twitter data\n","    tokenizer = BertTokenizer.from_pretrained('../bert-it/bert-it-vocab.txt')\n","else:\n","    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)\n","\n","df.dropna(subset=['x'], inplace=True)\n","\n","# Split the dataset into the various train/test/validation splits\n","train_raw = pd.DataFrame(df[df['split'] == 'train'])\n","test_raw = pd.DataFrame(df[df['split'] == 'test'])\n","val_raw = pd.DataFrame(df[df['split'] == 'val'])\n","\n","\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","# try and load the current ordinal encoder, this ensures that we're using the correct target mappings, if you wish to skip training and only evaluate the model\n","try:\n","    preprocessor_config = pd.read_pickle('preprocessor.pkl')\n","    targ_encoder = preprocessor_config.loc['targ_encoder']['x']\n","except:\n","    targ_encoder = OrdinalEncoder()\n","    preprocessor_config = pd.DataFrame()\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","def preprocess_data(df, fit=False):\n","    \"\"\"\n","    Preprocesses the raw text data to produce Input IDs, Input Masks and Segment IDs\n","\n","    Args:\n","        df (DataFrame): A dataframe containing a text column 'x' with the input data, and 'y' with target class labels (also a 'topic' column for UKP tasks)\n","        fit (bool, optional): Whether to fit the target encoder (False if evaluating, True if training a new model) Defaults to False.\n","\n","    Returns:\n","        DataFrame: df with the Input IDs ('x'), Input Masks ('input mask')  and Segment IDs ('segment ids') columns appended\n","    \"\"\"\n","    processed = pd.DataFrame()\n","    processed['x tokens'] = df['x'].apply(tokenizer.tokenize)\n","\n","     # Unfortunately the BERT tokenizer doesn't recognise special tokens such as [TAG] so splits them into [, tag, ]. So we have to repair these changes\n","    if 'Twitter' in MODE:\n","        special = {\n","            str(['[','rt',']']): ['[RT]'],\n","            str(['[','men',']']):['[MEN]'],\n","            str(['[','tag',']']):['[TAG]'],\n","            str(['[','tag',']']):['[TAG]'],\n","        }\n","\n","        def repair_special_tokens(x):\n","            for index in range(0, len(x)-2):\n","                if str(x[index:index+3]) in special:\n","                    x = x[0:index] + special[str(x[index:index+3])] + x[index+3:]\n","                    return repair_special_tokens(x)\n","            return x\n","\n","        processed['x tokens'] = processed['x tokens'].apply(repair_special_tokens)\n","\n","    \n","\n","    # If we are fitting the data, fit a new ordinal encoder and save it to a file\n","    if fit:\n","        targets_numpy = df['y'].to_numpy().reshape(-1, 1)\n","        processed['y'] = list(targ_encoder.fit_transform(targets_numpy).flatten())\n","        preprocessor_config = pd.DataFrame( pd.Series({'x': targ_encoder}, name='targ_encoder'))\n","        preprocessor_config.to_pickle('preprocessor.pkl')\n","    else:\n","        targets_numpy = df['y'].to_numpy().reshape(-1, 1)\n","        processed['y'] = list(targ_encoder.transform(targets_numpy).flatten())\n","    processed['y'] = processed['y'].astype(np.int32)\n","\n","    if \"UKP\" in MODE: # UKP data contains topic information, so must be split into two sentences (and each shortened to MAX_SEQ_LENGTH)\n","        def shorten(entity, fields):\n","            \"\"\"Shortens a number of fields, such that their sum is less than MAX_SEQ_LENGTH-3\n","\n","            Args:\n","                entity (pd.Series): A series containing the set of fields\n","                fields (list): list of fields to shorten\n","\n","            Returns:\n","                pd.Series: the shortened entity\n","            \"\"\"\n","            quota = MAX_SEQ_LENGTH-3\n","            new_entity = pd.Series(entity)\n","            \n","            def get_lengths():\n","                len_dict = dict()\n","                for field in fields:\n","                    len_dict[field] = len(new_entity[field])\n","                return len_dict\n","            len_dict = get_lengths()\n","            \n","            while sum(len_dict.values()) > quota:\n","                worst_field = max(len_dict, key=len_dict.get) \n","                new_entity[worst_field] = new_entity[worst_field][:-1]\n","                len_dict = get_lengths()\n","\n","            return new_entity\n","\n","        \n","        processed['topic tokens'] = df['topic'].apply(tokenizer.tokenize)\n","        processed = processed.apply(lambda e: shorten(e, ['topic tokens', 'x tokens']), axis=1)\n","        processed['tokens'] = processed.apply(lambda e: [\"[CLS]\"] + e['topic tokens'] + [\"[SEP]\"] + e['x tokens'] + [\"[SEP]\"], axis=1)\n","        processed['segment ids'] = processed.apply(lambda e: [0] * len([\"[CLS]\"] + e['topic tokens'] + ['[SEP]'])\n","                                                    + [1] * len(e['x tokens'] + [\"[SEP]\"])  , axis=1)\n","    else: # Twitter data contains topic information, so must be split into two sentences (and each shortened to MAX_SEQ_LENGTH)\n","        def shorten(x):\n","            \"\"\"Simply shortens the input sequence to be 3 tokens shorter than MAX_SEQ_LENGTH\n","            \"\"\"\n","            if len(x) > MAX_SEQ_LENGTH-3:\n","                return x[:MAX_SEQ_LENGTH-3]\n","            else:\n","                return x\n","\n","        processed['x tokens'] = processed['x tokens'].apply(shorten)\n","        processed['tokens'] = processed['x tokens'].apply(lambda x: [\"[CLS]\"] + x + [\"[SEP]\"])\n","        processed['segment ids'] = processed.apply(lambda e: [0] * len([\"[CLS]\"] + e['x tokens'] + ['[SEP]'])  , axis=1)\n","    \n","    # Compute the input Ids, input masks and segment ids\n","    processed['x'] = processed['tokens'].apply(tokenizer.convert_tokens_to_ids)\n","    processed['padding'] = processed['x'].apply(lambda x: [0] * (MAX_SEQ_LENGTH - len(x)))\n","    processed['input mask'] = processed['x'].apply(lambda x: [1] * len(x)) + processed['padding']\n","    processed['x'] = processed['x'] + processed['padding']\n","    processed['segment ids'] = processed['segment ids'] + processed['padding']\n","\n","    # Validate the lengths of the generated sequences\n","    assert processed['x'].apply(len).min()           == processed['x'].apply(len).max() == MAX_SEQ_LENGTH\n","    assert processed['input mask'].apply(len).min()  == processed['input mask'].apply(len).max() == MAX_SEQ_LENGTH\n","    assert processed['segment ids'].apply(len).min() == processed['segment ids'].apply(len).max() == MAX_SEQ_LENGTH\n","\n","    return processed\n","\n","\n","\n","def test_preprocess():\n","    \"\"\"\n","    Tests the functionality of preprocess data, according to the current MODE\n","    Assertions will throw exceptions if an error occurs\n","    \"\"\"\n","    test_input_a = \"that's good!\"\n","    test_input_b = \"no good\"\n","    tokenized_test_input_a = tokenizer.tokenize(test_input_a)\n","    tokenized_test_input_b = tokenizer.tokenize(test_input_b)\n","    assert tokenized_test_input_a == ['that', \"'\", 's', 'good', '!']\n","    assert tokenized_test_input_b == ['no', 'good']\n","\n","    #                        Tests       A             B\n","    test_data = pd.DataFrame(data=[\n","                                    [test_input_a,'good',1],\n","                                    [test_input_b, ' '.join(['bad']*MAX_SEQ_LENGTH),0]], # have 1 very long topic\n","\n","                    columns=['x', 'topic', 'y'])\n","\n","\n","    proc = preprocess_data(test_data, fit=True)\n","    res_a = proc.iloc[0]\n","    res_b = proc.iloc[1]\n","    assert (res_a['y'] == 0 and  res_b['y'] == 1) or (res_a['y'] == 1 and  res_b['y'] == 0)\n","    if 'Twitter' in MODE:\n","        a_len = len(tokenized_test_input_a)+2\n","        assert res_a['x'][0:a_len] == tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenized_test_input_a + ['[SEP]'])\n","        assert res_a['input mask'] == [1] * a_len + [0] * (MAX_SEQ_LENGTH - a_len)\n","        assert res_a['segment ids'] == [0] * MAX_SEQ_LENGTH\n","\n","        b_len = len(tokenized_test_input_b)+2\n","        assert res_b['x'][0:b_len] == tokenizer.convert_tokens_to_ids(['[CLS]'] + tokenized_test_input_b + ['[SEP]'])\n","        assert res_b['input mask'] == [1] * b_len + [0] * (MAX_SEQ_LENGTH - b_len)\n","        assert res_b['segment ids'] == [0] * MAX_SEQ_LENGTH\n","    else:\n","        a_len = len(tokenized_test_input_a)+4\n","        assert res_a['x'][0:a_len] == tokenizer.convert_tokens_to_ids(['[CLS]'] + ['good'] + ['[SEP]'] + tokenized_test_input_a + ['[SEP]'])\n","        assert res_a['input mask'] == [1] * a_len + [0] * (MAX_SEQ_LENGTH - a_len)\n","        assert res_a['segment ids'] == [0] * 3 + [1]*(len(tokenized_test_input_a)+1) + [0] * (MAX_SEQ_LENGTH-3-len(tokenized_test_input_a)-1)\n","\n","        b_len_minus_topic = len(tokenized_test_input_b) + 3\n","        b_len_topic = MAX_SEQ_LENGTH - b_len_minus_topic\n","        assert res_b['x'] == tokenizer.convert_tokens_to_ids(['[CLS]'] + ['bad'] * b_len_topic + ['[SEP]'] + tokenized_test_input_b + ['[SEP]'])\n","        assert res_b['input mask'] == [1] * MAX_SEQ_LENGTH\n","        assert res_b['segment ids'] == [0] * (b_len_topic  + 2)  + [1] * (b_len_minus_topic - 2)\n","    \n","    if 'Twitter' in MODE:\n","        test_input_c = \"[RT] [MEN] hello there [TAG]\"\n","        target = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"[RT]\", \"[MEN]\", \"hello\", \"there\", \"[TAG]\", \"[SEP]\"]]\n","        test_data = pd.DataFrame(data=[ [test_input_c,'good',1]], columns=['x', 'topic', 'y'])\n","        res = preprocess_data(test_data, fit=True).iloc[0]\n","        assert res['x'][:len(target)] == target\n","        assert res['input mask'] == [1] * len(target) + [0] * (MAX_SEQ_LENGTH - len(target))\n","        assert res['segment ids'] == [0] * MAX_SEQ_LENGTH\n","\n","    \n","\n","test_preprocess()\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","train = preprocess_data(train_raw, fit=True)\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","# Convert the dataset into tensors, for Pytorch compatability\n","def to_dataset(df):\n","    return TensorDataset(\n","        torch.tensor(df['x'].tolist(), dtype=torch.long),\n","        torch.tensor(df['input mask'].tolist(), dtype=torch.long),\n","        torch.tensor(df['segment ids'].tolist(), dtype=torch.long),\n","        torch.tensor(df['y'].tolist(), dtype=torch.long)\n","    )\n","train_data = to_dataset(train)\n","\n","# Use a random sampler to uniformly sample training examples\n","train_sampler = RandomSampler(train_data)\n","\n","# This constructs batches of samples, and pre-emptily sends them to the device\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE,  \n","    collate_fn=lambda x: tuple(x_.to(device) for x_ in torch.utils.data.dataloader.default_collate(x)))\n","\n","num_train_steps = int(np.ceil(len(train_data) / BATCH_SIZE))\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","# Load the default BERT model, and add a classification layer with 2 outputs (maybe 3 if training on the UKP corpus without binarisation)\n","model = BERTClassification(df['y'].unique().shape[0])\n","\n","if pre_trained_model_path is not None:\n","    pre_trained = torch.load(pre_trained_model_path)\n","    model.bert = pre_trained.bert # replace the default BERT implementation with the pre-trained one, leaving our classification layer alone\n","\n","model.to(device) # Move the model to the compute device\n","\n","# Many papers seem to modify the weight decay to prevent bias and layerNorms being decayed\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","# Define the BERTAdam optimiser, this optimises the hyperparameters such as the learning rate during training\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                            lr=LEARNING_RATE,\n","                            warmup=0.1,\n","                            t_total=num_train_steps*EPOCHS)\n","\n","\n","global_step = 0\n","\n","\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["loss_function = torch.nn.CrossEntropyLoss() # automatically recognises sparse labels unlike TF's implementation\n","\n","def epoch():\n","    global global_step\n","    model.train()\n","    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n","        input_ids, input_mask, segment_ids, label_ids = batch # unpack the tuple from the data loader\n","        out, bertOut, pooledOut, bert_hidden = model(input_ids, segment_ids, input_mask) # make our predictions for this batch\n","        loss = loss_function(out, label_ids) # measure how good the predictions were\n","        loss.backward() # backpropigate the error, updating the model\n","                        \n","        # Log the loss\n","        summary_writer.add_scalar('Loss/train', loss, global_step)\n","\n","        # iterate the optimizer and reset it's gradients\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        global_step += 1 \n","\n","    torch.save(model, log_dir + f\"{pd.Timestamp.now()} step: {global_step}\") # save the model from this epoch\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","\n","\n","def eval(eval_data):\n","    global global_step\n","    test = preprocess_data(eval_data)\n","    test_data = to_dataset(test)\n","    test_sampler = SequentialSampler(test_data) # Sequentially sample the data, no need to randomise when evaluating\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE,  \n","            collate_fn=lambda x: tuple(x_.to(device) for x_ in torch.utils.data.dataloader.default_collate(x))) # This constructs batches of samples, and pre-emptily sends them to the device\n","    model.eval() # disable the model's dropout\n","    accuracy = []\n","\n","    p = []\n","    t = []\n","    \n","    for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Evaluating\"):\n","\n","        with torch.no_grad(): # Don't keep track of the gradients for AutoGrad - we're not training\n","            out, bertOut, pooledOut, bert_hidden = model(input_ids, segment_ids, input_mask)\n","        \n","        # Transfer the predictions and targets to the CPU\n","        pred = out.detach().cpu().numpy()\n","        targ = label_ids.detach().cpu().numpy()\n","\n","        # Compute the accuracy by taking the argmax of the model outputs, then comparing the target and predicted labels\n","        pred_s = np.argmax(pred, axis=1).flatten()\n","        accuracy.append( (pred_s==targ.flatten()).mean() )\n","\n","        p += list(pred_s)\n","        t += list(targ.flatten())\n","\n","    print(f\"Accuracy: {mean(accuracy)}%\")\n","    summary_writer.add_scalar('Accuracy', mean(accuracy), global_step)\n","\n","    f1_micro = f1_score(t, p, average=\"micro\")\n","    print(f\"F1 Micro: {f1_micro}\")\n","    summary_writer.add_scalar('F1 Score Micro', f1_micro, global_step)\n","\n","    f1_macro = f1_score(t, p, average=\"macro\")\n","    print(f\"F1 Macro: {f1_macro}\")\n","    summary_writer.add_scalar('F1 Score Macro', f1_macro, global_step)\n","\n","\n","    test['prediction'] = p\n","    test.to_excel('pred.xlsx')\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Iteration:   0%|          | 0/80 [00:00<?, ?it/s]/home/w/Documents/KCL_Project/Clean/venv/lib/python3.9/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n","Iteration: 100%|██████████| 80/80 [00:15<00:00,  5.31it/s]\n","Evaluating: 100%|██████████| 13/13 [00:00<00:00, 23.67it/s]\n","Accuracy: 0.5115384615384615%\n","F1 Micro: 0.5120772946859904\n","F1 Macro: 0.5091446951376987\n","Iteration: 100%|██████████| 80/80 [00:14<00:00,  5.45it/s]\n","Evaluating: 100%|██████████| 13/13 [00:00<00:00, 23.45it/s]\n","Accuracy: 0.5458333333333333%\n","F1 Micro: 0.5458937198067633\n","F1 Macro: 0.35312499999999997\n","Iteration: 100%|██████████| 80/80 [00:15<00:00,  5.32it/s]\n","Evaluating: 100%|██████████| 13/13 [00:00<00:00, 23.64it/s]\n","Accuracy: 0.492948717948718%\n","F1 Micro: 0.4927536231884058\n","F1 Macro: 0.42725763524730564\n","Iteration: 100%|██████████| 80/80 [00:14<00:00,  5.41it/s]\n","Evaluating: 100%|██████████| 13/13 [00:00<00:00, 23.77it/s]\n","Accuracy: 0.49262820512820515%\n","F1 Micro: 0.4927536231884058\n","F1 Macro: 0.4709748083242059\n"]}],"source":["\n","for i in range(EPOCHS):\n","    epoch()\n","    eval(test_raw)\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","######################## Evaluation Performance ###########################\n","\n","Evaluating: 100%|██████████| 7/7 [00:00<00:00, 23.46it/s]\n","Accuracy: 0.6607142857142857%\n","F1 Micro: 0.6607142857142857\n","F1 Macro: 0.5987933634992457\n"]}],"source":["\n","print(\"\\n\\n\\n######################## Evaluation Performance ###########################\\n\")\n","eval(val_raw)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}