{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import re\n","from copy import copy \n","import random\n","import numpy as np\n","from transformers import BertTokenizer\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from statistics import mean\n","from torch.utils.data.dataloader import default_collate\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","import torch\n","from common import get_lists_of_sentences\n","from common import BERTClassification\n","from pytorch_pretrained_bert import BertAdam\n","\n","\n","# Set the seed for all random number generators, this makes the results more reproducible\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","\n","\n","\n","\n","\n","# Select the MODE to run in, these correspond to the 2 NSP tasks on the Twitter, and UKP datasets\n","# Simply paste the string in from the options bellow to change the task\n","# 'Twitter'\n","# 'UKP'\n","MODE = 'UKP'\n","\n","# Here the Hyper parameters for the model are chosen\n","MAX_SEQ_LENGTH = 64\n","BATCH_SIZE = 16\n","LEARNING_RATE = 2e-5\n","EPOCHS = 5\n","\n","# The bert model loaded that will be trained to perform the MSP task\n","BERT_MODEL = \"bert-base-uncased\" # bert-base-uncased bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese\n","\n","# Specify the path to the pre-trained Masked LM model\n","if MODE=='Twitter':\n","    MASKED_LM_PATH = \"Trained Models/Twitter/Masked LM/2022-04-01 06:10:01.498390 step: 88655\"\n","else:\n","    MASKED_LM_PATH = \"Trained Models/UKP/Masked LM/2022-04-01 00:39:32.682074 step: 25220\"\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","\n","tokenizer = None\n","\n","if MODE=='Twitter':\n","    # Import the Twitter Dataset (Un-Annotated)\n","    df = pd.read_pickle('../datasets/cleaned tweets.pkl')\n","    df.rename(columns={'text':'x'}, inplace=True)\n","    train_raw = df # Since we aren't analysing the performance of this model, just train on all the data\n","    # Load the tokenizer with the Twitter vocabulary\n","    tokenizer = BertTokenizer.from_pretrained('../bert-it/bert-it-vocab.txt')\n","    log_dir = 'Trained Models/Twitter/NSP/' # Write the logs and models to the appropriate folder\n","\n","else:\n","    # Import the UKP dataset\n","    df = pd.read_excel('../datasets/UKP Claim Detection.xlsx')\n","    # The UKP datasett has a couple URL's in, remove them\n","    url_regex = re.compile(\"\"\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\"\")\n","    df['x'] = df['x'].apply(lambda x: url_regex.sub(\"\", x) if not pd.isna(x) else x)\n","    \n","    train_raw = df # Since we aren't analysing the performance of this model, just train on all the data\n","\n","    # Load the tokenizer with the default vocabulary\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n","    log_dir = 'Trained Models/UKP/NSP/' # Write the logs and models to the appropriate folder\n","\n","\n","summary_writer = SummaryWriter(log_dir + 'runs') # Create the tensorboard logger\n","\n","# This is where we check if the system has a GPU, if so use it\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","# Convert the tokens to ID's and compute the appropriate segment ids and input masks that bert expects\n","def preprocess(labelled_sentence_pairs):\n","    \"\"\"\n","    Converts each pair of sentences of tokens into the corresponding input id's, segment id's and input masks\n","    Truncating each sequence to the MAX_SEQ_LENGTH\n","    \"\"\"\n","    df = pd.DataFrame()\n","    df['tokens'] = labelled_sentence_pairs.apply(lambda e: [\"[CLS]\"] + e['A'] + [\"[SEP]\"] + e['B'] + [\"[SEP]\"], axis=1)\n","    df['segment ids'] = labelled_sentence_pairs.apply(lambda e: [0] * len([\"[CLS]\"] + e['A'] + ['[SEP]'])\n","                                                                            + [1] * len(e['B'] + [\"[SEP]\"])  , axis=1)\n","    df['x'] = df['tokens'].apply(tokenizer.convert_tokens_to_ids)\n","    df['padding'] = df['x'].apply(lambda x: [0] * (MAX_SEQ_LENGTH - len(x)))\n","    df['input mask'] = df['x'].apply(lambda x: [1] * len(x)) + df['padding']\n","    df['x'] = df['x'] + df['padding']\n","    df['segment ids'] = df['segment ids'] + df['padding']\n","    df['y'] = labelled_sentence_pairs['y']\n","    return  df\n","\n","def generate_sentence_pairs(sentences):\n","    sentences = [x for x in sentences if len(x) > 1] # only keep lists of sentences with 2+ sentences\n","    \n","    #valid_pairs = sum([[x[i:i+2] for i in range(0, len(x)-1)] for x in sentences], [])\n","    valid_pairs = []\n","    for x in tqdm(sentences, desc=\"listing pairs of sentences\"):\n","        for i in range(0, len(x) - 1):\n","            valid_pairs.append(x[i:i+2])\n","\n","    # Pick rabdom pairs, provided the pair doesnt originate from the same original text\n","    invalid_pairs = []\n","    for invalid_sample_count in tqdm(range(len(valid_pairs)), desc=\"generating invalid pairs of sentences\"):\n","        sentence_list_a_index = random.randint(0, len(sentences)-1)\n","        \n","        # Pick a random index not equal to our other source\n","        sentence_list_b_index = random.randint(0, len(sentences)-2)\n","        if sentence_list_b_index >= sentence_list_a_index:\n","            sentence_list_b_index += 1\n","\n","        # select random sentences from each sentece list\n","        a = random.choice(sentences[sentence_list_a_index])\n","        b = random.choice(sentences[sentence_list_b_index])\n","\n","        invalid_pairs.append([a,b])\n","\n","    valid_df = pd.DataFrame(valid_pairs, columns=['A', 'B'])\n","    valid_df['y'] = 1\n","\n","    invalid_df = pd.DataFrame(invalid_pairs, columns=['A', 'B'])\n","    invalid_df['y'] = 0\n","\n","    return pd.concat([valid_df, invalid_df]).sample(frac=1)\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Tokenizing Text: 100%|██████████| 2/2 [00:00<00:00, 2074.33it/s]\n","Merging Sentences: 100%|██████████| 2/2 [00:00<00:00, 33554.43it/s]\n","listing pairs of sentences: 100%|██████████| 2/2 [00:00<00:00, 21675.99it/s]\n","generating invalid pairs of sentences: 100%|██████████| 2/2 [00:00<00:00, 43919.41it/s]\n"]}],"source":["\n","\n","\n","# Only test in the Twitter mode as that tests the full functionality of the get_lists_of_sentences function\n","if MODE == 'Twitter':\n","    def test_preprocess():\n","        \"\"\"\n","        Tests the functionality of get_lists_of_sentences and generate_sentence_pairs and preprocess methods\n","        Assertions will throw exceptions if an error occurs\n","        \"\"\"\n","        # Test the tokenization\n","        test_input_1 = \"[RT] [MEN] hello there. the quick brown fox jumped over the hill [TAG]\"\n","        test_input_2 = \"[MEN] [MEN] a fat cat on a mat. on a mat that cat had sat\"\n","        target_1 = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"[RT]\", \"[MEN]\", \"hello\", \"there\", \"[SEP]\", \"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"hill\", \"[TAG]\", \"[SEP]\"]]\n","        target_2 = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"[MEN]\", \"[MEN]\", \"a\", \"fat\", \"cat\", \"on\", \"a\", \"mat\", \"[SEP]\", \"on\", \"a\", \"mat\", \"that\", \"cat\", \"had\", \"sat\", \"[SEP]\"]]\n","        test_sentences = get_lists_of_sentences([test_input_1, test_input_2], ['.'], tokenizer, MODE, min_length=1)\n","        test_sentences_pairs = generate_sentence_pairs(test_sentences)\n","        processed_test_sentences = preprocess(test_sentences_pairs)\n","        true_processed_test_sentences = processed_test_sentences[processed_test_sentences['y'] == 1]\n","        \n","        assert true_processed_test_sentences.iloc[0]['x'][:len(target_1)] == target_1 or true_processed_test_sentences.iloc[0]['x'][:len(target_2)] == target_2\n","        assert true_processed_test_sentences.iloc[1]['x'][:len(target_2)] == target_2 or true_processed_test_sentences.iloc[1]['x'][:len(target_1)] == target_1\n","        \n","\n","    test_preprocess()\n","else:\n","    def test_preprocess():\n","        \"\"\"\n","        Tests the functionality of get_lists_of_sentences and generate_sentence_pairs and preprocess methods\n","        Assertions will throw exceptions if an error occurs\n","        \"\"\"\n","        # Test the tokenization\n","        test_input_1 = \"hello there. the quick brown fox jumped over the hill\"\n","        test_input_2 = \"a fat cat on a mat. on a mat that cat had sat\"\n","        target_1 = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"hello\", \"there\", \"[SEP]\", \"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"hill\", \"[SEP]\"]]\n","        target_2 = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"a\", \"fat\", \"cat\", \"on\", \"a\", \"mat\", \"[SEP]\", \"on\", \"a\", \"mat\", \"that\", \"cat\", \"had\", \"sat\", \"[SEP]\"]]\n","        test_sentences = get_lists_of_sentences([test_input_1, test_input_2], ['.'], tokenizer, MODE, min_length=1)\n","        test_sentences_pairs = generate_sentence_pairs(test_sentences)\n","        processed_test_sentences = preprocess(test_sentences_pairs)\n","        true_processed_test_sentences = processed_test_sentences[processed_test_sentences['y'] == 1]\n","        \n","        assert true_processed_test_sentences.iloc[0]['x'][:len(target_1)] == target_1 or true_processed_test_sentences.iloc[0]['x'][:len(target_2)] == target_2\n","        assert true_processed_test_sentences.iloc[1]['x'][:len(target_2)] == target_2 or true_processed_test_sentences.iloc[1]['x'][:len(target_1)] == target_1\n","        \n","\n","    test_preprocess()\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 3035.42it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 279660.74it/s]\n","listing pairs of sentences: 100%|██████████| 8834/8834 [00:00<00:00, 1004785.81it/s]\n","generating invalid pairs of sentences: 100%|██████████| 11118/11118 [00:00<00:00, 342094.52it/s]\n"]}],"source":["\n","\n","\n","\n","# Split each tweet into sentences and tokenize them, then merge sentences that are too short (minimum length of 10 by default)\n","sentences = []\n","if MODE=='Twitter':\n","    sentences = get_lists_of_sentences(train_raw['x'], ['.'], tokenizer, MODE)\n","else:\n","    sentences = get_lists_of_sentences(train_raw['x'], ['.', ';', ','], tokenizer, MODE) # Ideally we'd only use '.' but thanks to the small dataset use more\n","\n","\n","# given the list of lists of sentences, convert these into valid and invalid pairs\n","labelled_sentence_pairs = generate_sentence_pairs(sentences)\n","\n","\n","\n","# Before converting the sentence into IDs, make sure that together they are 3 tokens less than the MAX_SEQ_LENGTH\n","def shorten_sentence_pair(a, b, target):\n","    \"\"\"\n","    Shotens a pair of lists by removing 1 element the longest list repeatedly until the desired length is achieved\n","    \"\"\"\n","    while len(a) + len(b) > target:\n","        if len(a) > len(b):\n","            a = a[:-1]\n","        else:\n","            b = b[:-1]\n","    return (a,b)\n","# Apply the shortening function to the whole dataset\n","labelled_sentence_pairs[['A', 'B']] = labelled_sentence_pairs.apply(lambda e: shorten_sentence_pair(e['A'], e['B'], MAX_SEQ_LENGTH-3), axis=1, result_type=\"expand\")\n","\n","    \n","# Generate the input ids, input masks and segment ids\n","train = preprocess(labelled_sentence_pairs)\n","\n","\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","# Convert the dataset into tensors, for Pytorch compatability\n","tensor_dataset = TensorDataset(torch.tensor(train['x'].tolist()),\n","                                torch.tensor(train['input mask'].tolist()),\n","                                torch.tensor(train['segment ids'].tolist()),\n","                                torch.tensor(train['y'].tolist()))\n","\n","# Use a random sampler to uniformly sample training examples\n","train_random_sampler = RandomSampler(tensor_dataset)\n","\n","# This constructs batches of samples, and pre-emptily sends them to the device\n","train_dataloader = DataLoader(tensor_dataset, sampler=train_random_sampler, batch_size=BATCH_SIZE,  \n","    collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n","\n","num_train_steps = int(np.ceil(train.shape[0] / BATCH_SIZE))\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","\n","\n","\n","\n","\n","# Load the default BERT model, and add a classification layer with 2 outputs, either sentence \n","model = BERTClassification(2)\n","\n","model_masked_lm = torch.load(MASKED_LM_PATH) # Load the Masked LM Model we just trained\n","\n","model.bert = model_masked_lm.bert # Only replace the BERT embeddings and self-attention layers, we're adding a new classification layer\n","\n","\n","model.to(device) # Move the model to the compute device\n","\n","# Many papers seem to modify the weight decay to prevent bias and layerNorms being decayed\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                            lr=LEARNING_RATE,\n","                            warmup=0.1,\n","                            t_total=num_train_steps*EPOCHS)\n","\n","\n","global_step = 0\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","loss_function = torch.nn.CrossEntropyLoss() # automatically recognises sparse labels unlike TF's implementation\n","\n","def epoch():\n","    global global_step\n","    model.train()\n","    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\", total=num_train_steps)):\n","        input_ids, input_mask, segment_ids, label_ids = batch # unpack the tuple from the data loader\n","        out, bertOut, pooledOut, bert_hidden = model(input_ids, segment_ids, input_mask) # make our predictions for this batch\n","        loss = loss_function(out, label_ids) # measure how good the predictions were\n","        loss.backward() # backpropigate the error, updating the model\n","                        \n","        #  log the loss to tensorboard\n","        summary_writer.add_scalar('Loss/train', loss, global_step)\n","\n","        # iterate the optimizer and reset it's gradients\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        global_step += 1\n","\n","    torch.save(model, log_dir + f\"{pd.Timestamp.now()} step: {global_step}\") # save the model from this epoch\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","\n","\n","import numpy as np\n","from statistics import mean\n","from sklearn.metrics import f1_score\n","def eval(eval_data):\n","    global global_step\n","    \n","    sentences = []\n","    if MODE=='Twitter':\n","        sentences = get_lists_of_sentences(train_raw['x'], ['.'], tokenizer, MODE)\n","    else:\n","        sentences = get_lists_of_sentences(train_raw['x'], ['.', ';', ','], tokenizer, MODE) # Ideally we'd only use '.' but thanks to the small dataset use more\n","    labelled_sentence_pairs = generate_sentence_pairs(sentences)\n","    labelled_sentence_pairs[['A', 'B']] = labelled_sentence_pairs.apply(lambda e: shorten_sentence_pair(e['A'], e['B'], MAX_SEQ_LENGTH-3), axis=1, result_type=\"expand\")\n","    test = preprocess(labelled_sentence_pairs)\n","    tensor_dataset = TensorDataset(torch.tensor(test['x'].tolist()),\n","                                torch.tensor(test['input mask'].tolist()),\n","                                torch.tensor(test['segment ids'].tolist()),\n","                                torch.tensor(test['y'].tolist()))\n","\n","    test_random_sampler = RandomSampler(tensor_dataset)\n","\n","    test_dataloader = DataLoader(tensor_dataset, sampler=test_random_sampler, batch_size=BATCH_SIZE,  \n","        collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))  \n","\n","\n","\n","    model.eval()\n","    accuracy = []\n","\n","    p = []\n","    t = []\n","    \n","    for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Evaluating\"):\n","\n","        with torch.no_grad():\n","            out, bertOut, pooledOut, bert_hidden = model(input_ids, segment_ids, input_mask)\n","        \n","        # Transfer the predictions and targets to the CPU\n","        pred = out.detach().cpu().numpy()\n","        targ = label_ids.to('cpu').numpy()\n","\n","        pred_s = np.argmax(pred, axis=1).flatten()\n","        accuracy.append( (pred_s==targ.flatten()).mean() )\n","\n","        p += list(pred_s)\n","        t += list(targ.flatten())\n","\n","    print(f\"Accuracy: {mean(accuracy)}%\")\n","    summary_writer.add_scalar('Accuracy', mean(accuracy), global_step)\n","\n","    f1 = f1_score(t, p, average=\"micro\")\n","    print(f\"F1:       {f1}\")\n","    summary_writer.add_scalar('F1 Score', f1, global_step)\n","\n","    test['prediction'] = p\n","    test.to_excel('pred.xlsx')\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Iteration:   0%|          | 0/1390 [00:00<?, ?it/s]/home/w/Documents/KCL_Project/Clean/venv/lib/python3.9/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n","Iteration: 100%|██████████| 1390/1390 [04:31<00:00,  5.12it/s]\n","Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 2985.23it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 885526.58it/s]\n","listing pairs of sentences: 100%|██████████| 8834/8834 [00:00<00:00, 1091673.24it/s]\n","generating invalid pairs of sentences: 100%|██████████| 11118/11118 [00:00<00:00, 370220.80it/s]\n","Evaluating: 100%|██████████| 1390/1390 [01:05<00:00, 21.38it/s]\n","Accuracy: 0.690167865707434%\n","F1:       0.6901421118906278\n","Iteration: 100%|██████████| 1390/1390 [04:38<00:00,  4.99it/s]\n","Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 2962.13it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 860882.40it/s]\n","listing pairs of sentences: 100%|██████████| 8834/8834 [00:00<00:00, 678815.80it/s]\n","generating invalid pairs of sentences: 100%|██████████| 11118/11118 [00:00<00:00, 308502.23it/s]\n","Evaluating: 100%|██████████| 1390/1390 [01:06<00:00, 20.93it/s]\n","Accuracy: 0.7006594724220624%\n","F1:       0.7006206152185644\n","Iteration: 100%|██████████| 1390/1390 [04:29<00:00,  5.16it/s]\n","Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 2990.71it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 169076.34it/s]\n","listing pairs of sentences: 100%|██████████| 8834/8834 [00:00<00:00, 1116610.36it/s]\n","generating invalid pairs of sentences: 100%|██████████| 11118/11118 [00:00<00:00, 363052.45it/s]\n","Evaluating: 100%|██████████| 1390/1390 [01:02<00:00, 22.22it/s]\n","Accuracy: 0.7031774580335731%\n","F1:       0.7031840259039396\n","Iteration: 100%|██████████| 1390/1390 [04:16<00:00,  5.42it/s]\n","Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 3006.86it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 846766.75it/s]\n","listing pairs of sentences: 100%|██████████| 8834/8834 [00:00<00:00, 1055385.71it/s]\n","generating invalid pairs of sentences: 100%|██████████| 11118/11118 [00:00<00:00, 340009.27it/s]\n","Evaluating: 100%|██████████| 1390/1390 [01:01<00:00, 22.44it/s]\n","Accuracy: 0.7033273381294964%\n","F1:       0.7033189422558014\n","Iteration: 100%|██████████| 1390/1390 [04:27<00:00,  5.20it/s]\n","Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 3037.80it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 172888.67it/s]\n","listing pairs of sentences: 100%|██████████| 8834/8834 [00:00<00:00, 882033.93it/s]\n","generating invalid pairs of sentences: 100%|██████████| 11118/11118 [00:00<00:00, 355271.85it/s]\n","Evaluating: 100%|██████████| 1390/1390 [01:04<00:00, 21.61it/s]\n","Accuracy: 0.7070593525179856%\n","F1:       0.7070516279906458\n"]}],"source":["\n","for i in range(0, EPOCHS):\n","    epoch()\n","    eval(df['x'].sample(256))\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}