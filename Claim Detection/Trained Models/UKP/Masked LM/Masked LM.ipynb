{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","# Import all the dependencies\n","import pandas as pd\n","import re\n","from copy import copy \n","import random\n","import numpy as np\n","from pytorch_pretrained_bert.tokenization import BertTokenizer\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from statistics import mean\n","from transformers import BertTokenizer\n","from common import get_lists_of_sentences\n","import torch\n","from common import BERTClassification\n","from pytorch_pretrained_bert import BertAdam\n","\n","from torch.utils.data.dataloader import default_collate\n","from torch.utils.data import DataLoader, IterableDataset\n","\n","\n","\n","# Set the seed for all random number generators, this makes the results more reproducible\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","\n","\n","\n","\n","\n","# Select the MODE to run in, these correspond to the 2 Masked LM tasks on the Twitter, and UKP datasets\n","# Simply paste the string in from the options bellow to change the task\n","# 'Twitter'\n","# 'UKP'\n","MODE = 'UKP'\n","\n","# Here the Hyper parameters for the model are chosen\n","MAX_SEQ_LENGTH = 64\n","BATCH_SIZE = 16\n","LEARNING_RATE = 2e-5\n","EPOCHS = 5\n","\n","# The bert model loaded that will be trained to perform the Masked LM task\n","BERT_MODEL = \"bert-base-uncased\" # bert-base-uncased bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese\n","\n","\n","\n","\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","\n","tokenizer = None\n","\n","if MODE=='Twitter':\n","    # Import the Twitter Dataset (Un-Annotated)\n","    df = pd.read_pickle('../datasets/cleaned tweets.pkl')\n","    df.rename(columns={'text':'x'}, inplace=True)\n","    train_raw = df # Since we aren't analysing the performance of this model, just train on all the data\n","    # Load the tokenizer with the Twitter vocabulary\n","    tokenizer = BertTokenizer.from_pretrained('../bert-it/bert-it-vocab.txt')\n","    log_dir = 'Trained Models/Twitter/Masked LM/' # Write the logs and models to the appropriate folder\n","\n","else:\n","    # Import the UKP dataset\n","    df = pd.read_excel('../datasets/UKP Claim Detection.xlsx')\n","    # The UKP datasett has a couple URL's in, remove them\n","    url_regex = re.compile(\"\"\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\"\")\n","    df['x'] = df['x'].apply(lambda x: url_regex.sub(\"\", x) if not pd.isna(x) else x)\n","    \n","    train_raw = df # Since we aren't analysing the performance of this model, just train on all the data\n","\n","    # Load the tokenizer with the default vocabulary\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n","    log_dir = 'Trained Models/UKP/Masked LM/' # Write the logs and models to the appropriate folder\n","\n","\n","summary_writer = SummaryWriter(log_dir + 'runs') # Create the tensorboard logger\n","\n","# This is where we check if the system has a GPU, if so use it\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","# Convert the tokens to ID's and compute the appropriate segment ids and input masks that bert expects\n","def preprocess(sentences):\n","    \"\"\"\n","    Converts each sentence of tokens into the corresponding input id's, segment id's and input masks\n","    Truncating each sequence to the MAX_SEQ_LENGTH\n","    \"\"\"\n","    df = pd.DataFrame()\n","    df['tokens'] = [[\"[CLS]\"] + x + [\"[SEP]\"] for x in sentences]\n","    df['segment ids'] = [[0] * MAX_SEQ_LENGTH] * len(sentences)\n","    df['x'] = df['tokens'].apply(tokenizer.convert_tokens_to_ids)\n","    df['x'] = df['x'].apply(lambda x: x[0:MAX_SEQ_LENGTH-1] + tokenizer.convert_tokens_to_ids([\"[SEP]\"]) if len(x) > MAX_SEQ_LENGTH else x)\n","    df['length'] = df['x'].apply(len)\n","    df['padding'] = df['x'].apply(lambda x: [0] * (MAX_SEQ_LENGTH - len(x)))\n","    df['input mask'] = df['x'].apply(lambda x: [1] * len(x)) + df['padding']\n","    df['x'] = df['x'] + df['padding']\n","    return  df\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","# Only test in the Twitter mode as that tests the full functionality of the get_lists_of_sentences function\n","if MODE == 'Twitter':\n","    def test_preprocess():\n","        \"\"\"\n","        Tests the functionality of get_lists_of_sentences and preprocess\n","        Assertions will throw exceptions if an error occurs\n","        \"\"\"\n","        # Test the tokenization\n","        test_input = \"[RT] [MEN] hello there. the quick brown fox jumped over the hill [TAG]\"\n","        target_1 = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"[RT]\", \"[MEN]\", \"hello\", \"there\", \"[SEP]\"]]\n","        target_2 = [tokenizer.convert_tokens_to_ids(x) for x in [\"[CLS]\", \"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"hill\", \"[TAG]\", \"[SEP]\"]]\n","        test_sentences = get_lists_of_sentences([test_input], ['.'], tokenizer, MODE, min_length=1)  \n","        test_sentences = sum(test_sentences, [])\n","        processed_test_sentences = preprocess(test_sentences)\n","        assert processed_test_sentences.iloc[0]['x'][:len(target_1)] == target_1\n","        assert processed_test_sentences.iloc[1]['x'][:len(target_2)] == target_2\n","\n","        # Test the sentence merging is working\n","        test_sentences = get_lists_of_sentences([test_input], ['.'], tokenizer, MODE, min_length=10)  \n","        test_sentences = sum(test_sentences, [])\n","        processed_test_sentences = preprocess(test_sentences)\n","        target_3 = target_1[:-1] + target_2[1:]\n","        assert processed_test_sentences.iloc[0]['x'][:len(target_3)] == target_3\n","\n","    test_preprocess()\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Tokenizing Text: 100%|██████████| 76473/76473 [00:25<00:00, 3033.10it/s]\n","Merging Sentences: 100%|██████████| 76473/76473 [00:00<00:00, 289815.00it/s]\n"]}],"source":["\n","# Split each tweet into sentences and tokenize them, then merge sentences that are too short (minimum length of 10 by default)\n","sentences = []\n","if MODE=='Twitter':\n","    sentences = get_lists_of_sentences(train_raw['x'], ['.'], tokenizer, MODE)\n","else:\n","    sentences = get_lists_of_sentences(train_raw['x'], ['.', ';', ','], tokenizer, MODE) # Ideally we'd only use '.' but thanks to the small dataset use more\n","\n","\n","# Merge the list of lists of senteces into a single list of sentences\n","# Ideally we would use sum (shown bellow), but it does not always terminate for large inputs, instead we compute the sum ourselves\n","# sentences = sum(sentences, [])\n","acc = []\n","for x in sentences:\n","    acc += x\n","sentences = acc\n","\n","train_x = preprocess(sentences)\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","\n","mask_id = tokenizer.convert_tokens_to_ids([\"[MASK]\"])[0]\n","vocab_size = len(tokenizer.vocab)\n","\n","\n","def generate_sample(sample):\n","    \"\"\"\n","    Construct the generator that can convert a simple sentence into a sentence with 15% of the words masked off, according to Googles paper\n","    80% of those words we mask, replace with the [MASK] token\n","    10% with a random token\n","    10% unchanged\n","    In all cases add the masks word id's to the target vector\n","    \"\"\"\n","    sample_length = sample['length']\n","    samples_to_mask = int(0.15 * sample_length)\n","    indices = random.sample(range(1, sample_length-1), samples_to_mask) # take 1 off each end to account for the [CLS] and [SEP] tags\n","\n","    masked_input = copy(sample['x'])\n","\n","    # Targets are all the id's of the words we're masking\n","    y = [0] * vocab_size\n","    for index in indices:\n","        token = masked_input[index]\n","        y[token] = 1\n","\n","    # Mask of the indices we've chosen\n","    for index in indices:\n","        rand = random.random()\n","        if rand < 0.8:\n","            masked_input[index] = mask_id # replace token with [MASK] token\n","        elif rand < 0.9:\n","            masked_input[index] = random.randint(0, vocab_size-1) # replace with a random token\n","        # otherwise leave index unchanged\n","    \n","    return (torch.tensor(masked_input), \n","            torch.tensor(sample['input mask']), \n","            torch.tensor(sample['segment ids']),\n","            torch.tensor(y, dtype=torch.float))\n","\n","\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","# Create the data generator, rather than generate all the possible samples, saving lots of memory\n","# Also handles randomly sampling from the dataset\n","class MaskedLMData(IterableDataset):\n","    def __init__(self, df):\n","        super().__init__()\n","        self.df = df\n","    def __iter__(self):\n","        return iter(generate_sample(x) for _, x in self.df.sample(frac=1).iterrows())\n","\n","# This constructs batches of samples using the data generator, and pre-emptily sends them to the device\n","train_dataloader = DataLoader(MaskedLMData(train_x), batch_size=BATCH_SIZE,  \n","    collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n","\n","num_train_steps = int(train_x.shape[0] / BATCH_SIZE)\n","\n","\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","# Load the default BERT model, and add a classification layer with the same number of outputs as the size of out vocabulary\n","model = BERTClassification(vocab_size)\n","\n","model.to(device) # Move the model to the compute device\n","\n","# Many papers seem to modify the weight decay to prevent bias and layerNorms being decayed\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, #TODO change back to 0.001\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                            lr=LEARNING_RATE,\n","                            warmup=0.1,\n","                            t_total=num_train_steps*EPOCHS)\n","\n","\n","\n","global_step = 0\n","\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","\n","loss_function = torch.nn.BCELoss() # Use the BCE loss as we're doing multi-label classification\n","\n","def epoch():\n","    global global_step\n","    model.train()\n","    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\", total=num_train_steps)):\n","        input_ids, input_mask, segment_ids, label_ids = batch # unpack the tuple from the data loader\n","        out, bertOut, pooledOut, bert_hidden = model(input_ids, segment_ids, input_mask) # make our predictions for this batch\n","        loss = loss_function(out, label_ids) # measure how good the predictions were\n","        loss.backward() # backpropigate the error, updating the model\n","                        \n","        #  iterate the optimizer and reset it's gradients\n","        summary_writer.add_scalar('Loss/train', loss, global_step)\n","\n","        # iterate the optimizer and reset it's gradients\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        global_step += 1\n","\n","    torch.save(model, log_dir + f\"{pd.Timestamp.now()} step: {global_step}\") # save the model from this epoch\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","\n","\n","\n","\n","\n","def eval(eval_data):\n","    global global_step\n","\n","    sentences = get_lists_of_sentences(eval_data.iloc[0:256], ['.', ';', ','], tokenizer, MODE)\n","    sentences = sum(sentences, [])\n","\n","    test_x = preprocess(sentences)\n","    test_dataloader = DataLoader(MaskedLMData(test_x), batch_size=BATCH_SIZE,  \n","        collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n","\n","    model.eval()\n","    accuracy = []\n","\n","    accuracy_list = []\n","    \n","    for input_ids, input_mask, segment_ids, label_ids in tqdm(test_dataloader, desc=\"Evaluating\"):\n","\n","        with torch.no_grad():\n","            out, bertOut, pooledOut, bert_hidden = model(input_ids, segment_ids, input_mask)\n","\n","        # Transfer the predictions and targets to the CPU\n","        pred = out.detach().cpu().numpy()\n","        targ = label_ids.to('cpu').numpy()\n","        \n","        acc = []\n","        for batch_index in range(0, pred.shape[0]):\n","            p = pred[batch_index,:]\n","            t = targ[batch_index,:]\n","            target_count = int(t.sum()) # the sum of the target vector will equal the number tokens we masked\n","            target_indices = (-t).argsort()[:target_count] # calculate the top predicted targets\n","            predicted_indices = (-p).argsort()[:target_count]\n","\n","            accuracy = len(set.intersection(set(target_indices), set(predicted_indices))) / target_count\n","\n","            predicted_ones = np.zeros(p.shape)\n","            predicted_ones[predicted_indices] = 1\n","\n","            acc.append(accuracy)\n","\n","    \n","        accuracy_list.append(mean(acc))\n","\n","    # Only calculate accuracy, since there are 30k classes F1 is expensive\n","    accuracy = mean(accuracy_list)\n","    print(f\"Accuracy: {accuracy}%\")\n","    summary_writer.add_scalar('Accuracy', accuracy, global_step)\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["Iteration:   0%|          | 0/5043 [00:00<?, ?it/s]/home/w/Documents/KCL_Project/Clean/venv/lib/python3.9/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n","Iteration: 5044it [18:09,  4.63it/s]\n","Tokenizing Text: 100%|██████████| 768/768 [00:00<00:00, 3222.24it/s]\n","Merging Sentences: 100%|██████████| 768/768 [00:00<00:00, 880597.45it/s]\n","Evaluating: 49it [00:04, 11.17it/s]\n","Accuracy: 0.08590308147068351%\n","Iteration: 5044it [17:04,  4.92it/s]\n","Tokenizing Text: 100%|██████████| 768/768 [00:00<00:00, 3270.37it/s]\n","Merging Sentences: 100%|██████████| 768/768 [00:00<00:00, 729774.69it/s]\n","Evaluating: 49it [00:04, 10.18it/s]\n","Accuracy: 0.0892270003239391%\n","Iteration: 5044it [17:11,  4.89it/s]\n","Tokenizing Text: 100%|██████████| 768/768 [00:00<00:00, 3237.86it/s]\n","Merging Sentences: 100%|██████████| 768/768 [00:00<00:00, 792819.46it/s]\n","Evaluating: 49it [00:04, 11.24it/s]\n","Accuracy: 0.09726119614512471%\n","Iteration: 5044it [17:09,  4.90it/s]\n","Tokenizing Text: 100%|██████████| 768/768 [00:00<00:00, 3178.85it/s]\n","Merging Sentences: 100%|██████████| 768/768 [00:00<00:00, 765682.31it/s]\n","Evaluating: 49it [00:04, 10.52it/s]\n","Accuracy: 0.09486050372529964%\n","Iteration: 100%|█████████▉| 5040/5043 [17:31<00:00,  4.87it/s]Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n","Iteration: 100%|█████████▉| 5041/5043 [17:31<00:00,  4.91it/s]Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n","Iteration: 100%|█████████▉| 5042/5043 [17:31<00:00,  4.90it/s]Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n","Iteration: 100%|██████████| 5043/5043 [17:32<00:00,  4.83it/s]Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n","Iteration: 5044it [17:32,  4.79it/s]\n","Tokenizing Text: 100%|██████████| 768/768 [00:00<00:00, 3240.87it/s]\n","Merging Sentences: 100%|██████████| 768/768 [00:00<00:00, 768055.67it/s]\n","Evaluating: 49it [00:04, 11.19it/s]Accuracy: 0.09662394719792679%\n","\n"]}],"source":["\n","for i in range(EPOCHS):\n","    epoch()\n","    eval(df['x'])\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}